{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Blog","text":""},{"location":"2023/07/23/navigating-ai-as-an-engineering-leader/","title":"Navigating AI as an Engineering Leader","text":"<p>As an Engineering Leader in today's tech-driven environment, mastering the dynamics of AI is no longer an option but a necessity. Understanding AI in all its facets - the underlying technologies such as machine learning, deep learning, and natural language processing, their practical applications, and their broader implications - is your first essential step. Being aware of the ethical aspects and regulatory landscape around AI is equally important.</p> <p>However, not every challenge in your organization calls for an AI solution. You must be able to distinguish the issues where AI can provide substantial value. An integral part of this process is conducting a thorough cost-benefit analysis, ensuring that the potential gains from AI implementation are worth the investment of resources.</p> <p>When it comes to integrating AI solutions, the decision of building in-house versus buying ready-made can be a tough call. In-house development provides customization and control but requires substantial time, skills, and resources. On the other hand, purchasing ready-made AI solutions is quicker but might not perfectly suit your specific requirements. Weighing your team's AI readiness, available resources, and strategic objectives will help you make the right decision.</p> <p>One of your key roles as an Engineering Leader is to cultivate a team that's ready to embrace AI. This involves hiring or training personnel with the right skills and fostering a culture that promotes ongoing learning, innovation, and ethical AI practices.</p> <p>AI technologies are data-hungry and require robust computational resources. Therefore, you need to establish stringent data governance protocols and ensure that your tech infrastructure is up to the task. Depending on your specific needs, on-premise, cloud-based, or hybrid solutions could be the best fit.</p> <p>Once you've identified the right AI solutions and prepared your team, it's time to implement. Start with pilot projects, carefully monitor their performance and ethical adherence, and adjust as needed. Be aware of potential changes in AI model performance over time due to evolving data patterns.</p> <p>After successful pilot projects, the challenge of scalability arises. You must consider the interoperability of AI solutions with existing systems, ensure the transparency of AI algorithms, and work towards maintaining user trust as AI is integrated more broadly into your operations.</p> <p>As an Engineering Leader, your ultimate goal should be leveraging AI responsibly and effectively, enhancing your team's capabilities, and adding significant value to your organization, all while adhering to ethical standards.</p>"},{"location":"2023/05/03/blue-api/","title":"Blue API","text":"<p>In today's post, I will talk a little bit about Blue API, our public-facing, as well as internal APIs. Blue API allows you to programmatically access our services such as Ripple and WavePro. It uses protocol buffers for its service and message definitions, and gRPC for implementation and server/client stub generation. It also uses grpc-gateway for proxying JSON/REST requests to gRPC, and generating OpenAPI documentation. This way, you have the option to use our APIs using either gRPC or JSON/REST, or both.</p>"},{"location":"2023/05/03/blue-api/#structure","title":"Structure","text":"<p>Blue API starts with our API definitions. You can find the repository here. Once compiled, it will generate our main SDK along with the other SDKs we support. (By the way, if you're a user and have a particular programming language you want supported, contact us at support@alphaus.cloud and we will try to prioritize it.) The build also generates our OpenAPI reference documentation here. Our team then uses the generated SDK, blue-sdk-go (we use Go as our main programming language), to implement the necessary gRPC services in the backend. As a client, you can either call our gRPC services directly using the supported SDKs, or call our HTTP proxies through an HTTP client such as <code>curl</code>, or any HTTP library which is part of most modern programming languages.</p>"},{"location":"2023/05/03/blue-api/#authentication","title":"Authentication","text":"<p>Blue API uses client credentials (tokens) for authentication. You can check this detailed guide for more information.</p>"},{"location":"2023/05/03/blue-api/#usage","title":"Usage","text":"<p>Here's an example Go code on how to use the SDK to call our gRPC services directly:</p> <pre><code>ctx := context.Background()\nclient, _ := iam.NewClient(ctx)\ndefer client.Close()\nout, err := client.WhoAmI(ctx, &amp;iam.WhoAmIRequest{})\nlog.Println(out, err)\n</code></pre> <p>Here's another example using <code>curl</code> (uses our bluectl tool to generate the token):</p> <pre><code>$ curl -H \"Authorization: Bearer $(bluectl token)\" \\\n  https://api.alphaus.cloud/m/blue/iam/v1/whoami | jq\n</code></pre>"},{"location":"2023/05/03/blue-api/#closing","title":"Closing","text":"<p>At the moment, Blue API is still a work in progress. Most of the APIs currently supported in Ripple and WavePro are still not available. In the meantime, you can still use our JSON/REST APIs here. We plan to upgrade as many of our JSON/REST APIs as possible over to gRPC as it is significantly more efficient in terms of throughput and CPU usage compared to JSON/REST API. However, we don't intend to deprecate our JSON/REST APIs once the transition is completed. You should be able to use both.</p>"},{"location":"2023/05/18/bluectl/","title":"bluectl","text":"<p>In today's post, I will introduce <code>bluectl</code>, the official command line interface (CLI) for Alphaus services. To those who prefer CLIs (that includes me), we provide <code>bluectl</code> for interacting with our services. It uses the same API that powers our Ripple/Wave[Pro]/Aqua UI consoles.</p>"},{"location":"2023/05/18/bluectl/#installation","title":"Installation","text":"<p>You can install <code>bluectl</code> using Homebrew (MacOS, Linux, and Windows through WSL/2). Run the command below in a terminal: <pre><code>$ brew install alphauslabs/tap/bluectl\n</code></pre></p>"},{"location":"2023/05/18/bluectl/#authentication","title":"Authentication","text":"<p><code>bluectl</code> uses API client credentials for authentication. You can generate your API credentials either from Ripple under \"Tools &gt; API Access Tokens\", or Wave[Pro] under \"Settings &gt; API Access Tokens\".</p> <p>To validate your credentials with <code>bluectl</code>, run the command below (replace the <code>{client-*}</code> part with your actual client id and client secret values): <pre><code>$ bluectl whoami --client-id {client-id} --client-secret {client-secret}\n</code></pre></p> <p>If successful, it will output some information about the authenticated user.</p>"},{"location":"2023/05/18/bluectl/#environment-variables","title":"Environment variables","text":"<p>You can also store your credentials as environment variables instead of typing them everytime you run a command. Check out the \"Environment setup\" section here.</p> <p>With environment variables set, you should now be able to run any <code>bluectl</code> commands without the <code>--client-id</code> and <code>--client-secret</code> flags. <pre><code>$ bluectl whoami\n</code></pre></p>"},{"location":"2023/05/18/bluectl/#configuration-file","title":"Configuration file","text":"<p><code>bluectl</code> also supports authentication using a configuration file located in <code>$HOME/.config/alphaus/config.toml</code>.</p> <pre><code>[default]\nclient-id = 'sample-id'\nclient-secret = 'sample-secret'\n\n[beta]\nclient-id = 'sample-id'\nclient-secret = 'sample-secret'\nauth-url = 'https://loginnext.alphaus.cloud/ripple/access_token'\n</code></pre> <p>You can select a profile using the <code>--profile</code> flag. For example: <pre><code>$ bluectl whoami --profile beta\n</code></pre></p> <p>If the configuration file exists and the <code>[default]</code> profile is set, <code>bluectl</code> will use that credentials. In this case, the <code>--profile default</code> flag can be omitted. If both environment variables and the <code>[default]</code> profile are present, <code>bluectl</code> will use the <code>[default]</code> profile.</p> <pre><code># This usage...\n$ bluectl whoami --profile default\n\n# is the same as...\n$ bluectl whoami\n</code></pre>"},{"location":"2023/05/18/bluectl/#usage","title":"Usage","text":"<p>Finally, you can explore some of <code>bluectl</code>'s available supported commands by running: <pre><code># Check out the main commands:\n$ bluectl -h\n\n# More information on a specific subcommand:\n$ bluectl {subcommand} -h\n</code></pre></p> <p>We also have several guides that use <code>bluectl</code> as the main tool. You can check the links below for more information.</p> <p>Registering AWS payer accounts using bluectl Querying AWS costs using bluectl Adding cost modifiers to the AWS calculator using bluectl</p>"},{"location":"2024/10/16/how-alphaus-saves-on-costs-by-stitching-storage/","title":"How Alphaus saves on costs by \"stitching storage\"","text":"<p>(Repost from https://flowerinthenight.com/blog/2024-07-24-spillover-store/)</p> <p>One of Alphaus' data processing pipelines ingests around 10TB of client financial data per day. The processing engine is running on GKE with around 80-100 (depending on what week of the month) pods sharing the total workload. Each pod has around 10GB of memory and 30GB of attached storage. The consistency of this load allowed us to purchase enough Committed Use Discounts (CUDs) for the underlying VMs to save on compute costs.</p> <p>These pod resource limits are usually enough 80% of the time. However, since late last year, some of the accounts have datasets that are way, way beyond these limits causing persistent OOMKilled events.</p> <p>Our first stop-gap solution was to increase the memory limit. The trouble was, even with 20GB+ of memory wasn't really enough for some of the input datasets. And on top of this, GKE's cluster autoscaler also started increasing the VM sizes to those which we don't have CUDs for. Suffice it to say, it increased our monthly cloud spend to about +20% while delaying the overall processing time due to pods crashing (and restarting).</p> <p>We tried other solutions. One was using local files which required increasing the size of the attached storage. While cost-effective, the performance drop was significant mainly because most of the datasets that were well within the memory limit were now moved to disk as well. We also tried using the database we are currently using which turned out to be worse in terms of perfomance and costs. We also tried to use our cache layer (named Jupiter) which was very performant but prohibitively expensive.</p> <p>Enter Spill-over Store (SoS), our current solution. Inspired by Apache Ignite's design, the idea is to stitch together the already available memory and storage across the running pods, providing an ad-hoc, on-demand storage for really big datasets.</p> <p>From the image above, the pod that is assigned to load a huge dataset will exhaust its local memory first, then \"spill over\" to its local disk, then another pod's memory (using gRPC streaming), then disk, and so on. Thus, our example 100GB dataset will utilize around 4 pods in total within the cluster.</p> <p>This solution allowed us to actually revert back to our original pod resource limits. Both disk and network performance are acceptable (we don't use GCP's Tier 1 network) and still within our SLA as the solution only applies to about 20% of the ingestion pipeline. The majority still uses local in-memory processing.</p> <p>As Alphaus grows (and therefore ingests more and more data) and serve more clients, maybe we will eventually end up using Apache Ignite or some other off-the-shelf distributed solutions, but as of now, SoS works. With that said, if you have a cost-effective (and better) product/solution in mind, please feel free to contact us. We'd love to talk.</p> <p>Finally, you can find SoS's implementation here (if you're interested).</p>"},{"location":"2023/08/09/navigating-the-finops-landscape-lessons-from-the-finops-x-conference/","title":"Navigating the FinOps Landscape: Lessons from the FinOps X Conference","text":"<p>(Repost from https://alphaus.cloud/en/blog/navigating-the-finops-landscape-lessons-from-the-finops-x-conference/)</p> <p>Our recent attendance at the FinOps X conference in San Diego provided a valuable and enlightening experience. Immersed in the dynamic world of FinOps, the event featured engaging conversations with thought leaders, insightful talks and abundant networking opportunities. It proved to be an enriching experience that not only broadened our perspectives but also presented us with new avenues for growth and exploration.</p> <p>The conference attracted an impressive turnout of 1,200 participants, which was three times (3x) higher than the previous year, marking the largest gathering of FinOps professionals to date. The organizers succeeded in securing 150 speakers from companies like Walmart, Apple, Disney, and JPMorgan Chase. The conference was not just for the experts, but newcomers too. It was about helping everyone grow their skills, not just selling tech.</p> <p>Over the years, the FinOps community has grown exponentially, forming a global family of 11,000 members across 111 countries. A noteworthy statistic highlights the community\u2019s significance, with nine out of the top ten Fortune companies participating in the FinOps Foundation programs. These programs have successfully trained a total of 13,000 individuals, with 6,500 earning professional certifications. Additionally, the conference displayed a commitment to inclusivity through its scholarship program, which awarded $350,000 in support. Further reinforcing the commitment to growth, the event introduced a jobs board to address the rising demand in the expanding FinOps job market.</p> <p>During the conference, we heard an interesting story from HSBC\u2019s exec, Natalie Daley. She shared the bank\u2019s rollercoaster ride over the last four years to upgrade their FinOps game. After their cloud use spiked by 400% in the first year, they decided it was time to get serious about managing their cloud costs and multi-cloud strategy. She shared how they nurtured their FinOps team, made everything transparent, and got everyone to join in. Even as they moved forward, they tied FinOps optimization with cutting down carbon emissions, matching their sustainability goals.</p> <p>Breakout sessions allowed us to delve deeper into specific FinOps topics, one of which, titled \u201cAdvanced Approach to Prepay Amortization and Rate Blending,\u201d proved especially valuable for our work at Alphaus. We\u2019re currently integrating this feature into our FinOps Enabler tool, OCTO. This enhancement aims to augment our existing \u2018True Unblended\u2019 feature by allowing enterprises to equitably distribute savings benefits across a diverse range of usage types.</p> <p>The conference also proved to be an exceptional networking opportunity, enabling fruitful conversations with potential clients, industry peers, and seasoned professionals. This firsthand exchange provided a comprehensive overview of the prevailing trends and developments within the FinOps landscape.</p>"},{"location":"2023/08/09/navigating-the-finops-landscape-lessons-from-the-finops-x-conference/#key-takeaways","title":"Key Takeaways","text":"<p>1) FinOps is a big deal: As businesses are moving to the cloud, getting good at FinOps is a must, not just a nice-to-have.</p> <p>2) Cloud Complexity: The keynote talked about how managing and optimizing cloud costs can be tricky. Understanding these complexities is step one to manage cloud costs effectively.</p> <p>3) Unit Economics: FinOps is not just about cutting costs; it\u2019s about upping profits. Costs don\u2019t tell the whole story unless you see the value they create. This backs our strategy.</p> <p>4) Be Transparent: There was a lot of talk about the need for transparency. This goes hand in hand with our aim to make cost visibility a key feature in OCTO, our new product.</p> <p>5) Automation\u2019s Role: There was a mention of how automation/AI can help reduce mistakes, save time, and boost accuracy in FinOps.</p> <p>6) Tagging is key: The topic of tagging came up a lot, especially its role in making multi-cloud cost visibility easy.</p> <p>7) Teamwork is a Must: They stressed the need for finance, IT, and business units to work together for FinOps success.</p> <p>8) Future Trends: The keynote also talked about what\u2019s next in FinOps, like machine learning, predictive analytics, and the impact of the newly announced FOCUS project on cloud billing data standardization. We are grateful for the opportunity to participate as one of the contributors in the development of FOCUS.</p> <p>Overall, the FinOps X conference provided valuable insights and invaluable connections, affirming the importance of FinOps in today\u2019s cloud-dominated landscape. We are grateful for the opportunity to have participated and are excited to apply these learnings to further enhance our FinOps solutions at Alphaus.</p>"},{"location":"2023/08/09/navigating-the-finops-landscape-lessons-from-the-finops-x-conference/#author","title":"Author","text":"<p>Mohd Atasha Alias (LinkedIn) Regional Director (APEJ) at Alphaus Cloud</p>"},{"location":"2025/02/05/octo-recognized-as-a-finops-certified-platform/","title":"Octo Recognized as a FinOps Certified Platform","text":"<p>(Not really anything technical, but the engineering team is equally proud of this.)</p> <p>Tokyo, Japan (February 3, 2025) - Alphaus Inc., a leading innovator in cloud financial management, is proud to announce that its flagship platform, Octo, has been officially recognized as a FinOps Certified Platform by the FinOps Foundation. This certification establishes Octo as the first and only FinOps Certified Platform in Japan and Southeast Asia, reinforcing Alphaus\u2019 position as a trailblazer in cloud cost management solutions.</p> <p>Original press release here.</p>"},{"location":"2023/05/29/announcing-octo/","title":"Announcing OCTO","text":"<p>I am excited to announce that the public beta of our new product, Octo, is now available!</p> <p>We've been working hard on this product for the past few months, and we're really excited to share it with you. We're still under development, but we've already received great feedback from our early beta testers.</p> <p>In the public beta, you'll have access to all of the features of Octo, but there may be some bugs or limitations. We're committed to making Octo the best it can be, so your feedback is essential.</p> <p>To book a demo, please visit here.</p> <p>We can't wait to hear what you think!</p>"},{"location":"2023/05/29/announcing-octo/#press-release-copy","title":"(Press release copy)","text":"<p>Alphaus Inc. (https://alphaus.cloud/en/), the market leader in cloud financial management (CFM) solutions in Japan with a growing regional presence in Southeast Asia, is extending its suite of SaaS solutions to cater to SMEs and large enterprises running on cloud infrastructure.</p> <p>Having successfully delivered huge cost savings to companies in Japan and Southeast Asia through its proven solutions for managed service providers (MSP), the company is now globally introducing Octo, a SaaS-based FinOps platform for any business that uses cloud infrastructure for its operations. Beyond cost savings, Octo enables finance, FinOps and DevOps teams to work together effectively to achieve the best outcomes for the business. Moreover, Octo integrates effectively with a multitude of third-party solutions, acting as the connecting hub that brings them all together. This seamless integration enhances user experience, solidifying Octo as an essential tool for end-users.</p> <p>Companies interested in minimizing wasted cloud spend, saving costs and maximizing their ROI are now invited to sign up for the waitlist to experience the many benefits of Octo.</p> <p>Octo enables cost aggregation by account, service, or tags with centralized account management for AWS, Azure, and Google Cloud Platform (GCP). Smart visualization through project-specific dashboards provides a clear view and understanding of all cloud costs. With advanced tag management, Octo then intelligently distributes project costs and allocates credits and savings through its optimization capabilities. It helps businesses with a comprehensive cost optimization cycle covering numerous daily operations and enabling auto-pilot cost optimization, together with detailed metrics for gauging success of plans and actions.</p> <p>\"Due to the growing complexity of cloud technologies, most businesses find it a significant challenge to track cloud usage and associated costs across the organization. This lack of clear visibility and understanding of cloud costs negates the potential benefits and savings for companies by using the cloud,\" said Hajime Hirose, CEO of Alphaus. \"We aim to solve this problem at scale with a versatile, comprehensive, and user-centric solution like Octo, and help any business- whether it is a SME or a large-enterprise- to fully realize the benefits of using the cloud. Beyond understanding cloud spend, we want to empower businesses to seamlessly manage and optimize that spend with appropriate allocation, generate savings, and make a tangible impact on their bottom line.\"</p> <p>Don't miss out on the opportunity to save on cloud costs and maximize your ROI. Sign up for the waitlist today and be among the first to experience the many benefits of Octo. Visit https://www.alphaus.cloud/en/octo.</p>"},{"location":"2023/05/29/announcing-octo/#about-alphaus-inc","title":"About Alphaus Inc.","text":"<p>Alphaus (https://alphaus.cloud/), a VC-backed tech start-up on a mission to simplify cloud computing for everyone, specializes in Cloud Financial Management (CFM) solutions. The company enables cloud services partners and other businesses to understand, manage and optimize complicated cloud spend, billings and resource allocation for maximizing ROI on their investments in the cloud. Alphaus provides a suite of Software-as-a-Service (SaaS) solutions for multi-cloud management supporting AWS, Microsoft Azure, and Google Cloud.</p> <p>Founded in 2015, Alphaus Inc. is backed by reputed investors like DNX Ventures, NTT DoCoMo Ventures, Mitsubishi UFJ Capital, Archetype Ventures, Accord Ventures, and 500 Global. The company's roster of clients includes NTT Data, Nomura Research Institute (NRI), and ISI-Dentsu. Headquartered in Japan, Alphaus has a rapidly growing Global Delivery Centre and Regional Headquarter in Kuala Lumpur Malaysia to support its rapid expansion in the Asia Pacific and Oceania regions.</p>"},{"location":"2023/05/29/announcing-octo/#media-contact","title":"Media Contact","text":"<p>Hajime Hirose, CEO Tel: +81.70.3173.7354 E-mail: hajime.hirose@alphaus.cloud</p>"},{"location":"2023/04/24/product-introduction/","title":"Product introduction","text":"<p>At Alphaus, we have developed two products, Ripple and WavePro, both aimed at assisting users in managing their cloud costs effectively. These products focus on providing the necessary tools and insights to help users monitor and control their cloud expenses efficiently.</p> <p>Ripple has been specifically tailored for Managed Service Providers (MSPs), providing a cohesive, user-friendly multi-cloud console that streamlines invoicing and cost management for their clients. WavePro, on the other hand, serves as a versatile multi-cloud cost visualization and reporting tool, often bundled with Ripple, primarily targeted at MSP clients, and includes the added benefit of white labeling options. Together, these tools create a comprehensive solution that caters to the unique needs of MSPs and their clients, enhancing overall cloud cost management and reporting efficiency.</p>"},{"location":"2023/04/24/product-introduction/#ripple","title":"Ripple","text":"<p>Ripple is an invoicing software tool that facilitates efficient cost analysis, visualization, and optimization for MSPs. It centers around the concept of billing groups: grouping of cloud accounts. Most of the features in Ripple relate to billing groups. Once configured, it starts with the ingestion of billing data from AWS, Azure, and GCP.</p> <ul> <li>For AWS, CUR csv files exported to S3;</li> <li>For Azure, cost and usage data queried through their Partner Center API;</li> <li>For GCP, billing data exported to BigQuery.</li> </ul> <p>Once data is ingested into Ripple, it undergoes data processing. This involves various data transformations and aggregations to generate meaningful insights. One unique feature of Ripple, is the capability to perform corrective recalculations specifically for AWS billing data. We call this TrueUnblended. TrueUnblended-based calculations are particularly valuable to MSPs since they allow for accurate and detailed visibility of account costs without worrying about Reserved Instances and/or Savings Plans discount sharing across different organizations.</p> <p>Once data is ready for invoicing, Ripple can handle all invoicing requirements, including chargebacks, custom charges, premiums, custom pricing, customizable invoice PDF layout, and much more. Ripple supports multiple currencies for invoicing, making it convenient for MSPs who work with clients in different regions.</p> <p>In addition to its invoicing capabilities, Ripple also offers valuable features such as recommendations for RI and SP purchases, as well as usage pattern insights. This enables MSPs to optimize their cloud usage, reduce costs, and maximize their return on investment. Overall, Ripple is a powerful tool that helps MSPs streamline their billing processes and make data-driven decisions to improve their cloud financial management.</p>"},{"location":"2023/04/24/product-introduction/#wavepro","title":"WavePro","text":"<p>WavePro is a powerful visualization and reporting tool that complements Ripple's invoicing capabilities. Designed for MSP clients, WavePro features are tied to billing groups. A client can have one or multiple billing groups, depending on their organizational structure. Think of WavePro as a multi-cloud version of AWS Cost Explorer, providing clients with valuable insights into their cloud usage and costs.</p> <p>It's important to note that WavePro doesn't have direct access to the client's cloud environment. Instead, it relies on Ripple as its primary source of information. However, we are currently developing an option to allow WavePro users to set up API access to their cloud environments. With this feature, WavePro can provide additional value, such as RI/SP coverage and recommendations, forecasting, and more.</p> <p>Overall, WavePro is a valuable tool that helps MSP clients gain better visibility into their cloud usage and costs, enabling them to optimize their cloud resources, reduce costs, and maximize their return on investment.</p>"},{"location":"2023/04/24/product-introduction/#conclusion","title":"Conclusion","text":"<p>Ripple/WavePro is a proven and trusted cloud financial management solution that has become a leading choice for MSPs in Japan. We are excited to announce our expansion into the Asia Pacific region, with notable MSPs in Singapore and Malaysia already on board.</p> <p>If you're curious about how Ripple/WavePro can support your cloud financial management needs, we welcome you to contact us at https://alphaus.cloud/contact-us-en/. Our team is happy to answer any questions you may have and provide more information about our solution.</p>"},{"location":"2024/05/13/the-divs-model/","title":"The DIVS model","text":"<p>In this blog post, we present the DIVS model, a lightweight agile process designed for startups with small teams. This is the methodology we use within Alphaus.</p>"},{"location":"2024/05/13/the-divs-model/#background","title":"Background","text":"<p>As a mid-to-late stage startup (started 2015), we had our fair share of using different development processes within our teams over the years. I remember when I joined Alphaus, the current process didn't really have a name. Which was okay since we only had less than five engineers. We had a physical Kanban board, we met every Monday morning to discuss what we did the previous week along with the actual progress, and what we need to do for the week as well. It was simple, effective, and didn't really get in the way.</p> <p>Now, or at the time of this writing at least, we have four teams with about 5-9 engineers each across multiple countries, doing full-remote development. Before DIVS, our recurring pattern of process usually starts with Scrum, then tweak it a bit to become some sort of a variation of Scrumban, but usually lighter weight. We would call it \"Scrumban\" or just \"Scrum\" even though it's really not. And this really went on even when the company pivoted from our previous product to Ripple. There were some efforts into standardizing our process in the past but I would consider them unsuccessful since interestingly, when we started our newest product, Octo, the new team still went through our usual pattern: started with Scrum, then ended up with a not-Scrum-but-still-called-Scrum process. Personally, I don't really mind these dynamics too much since for me, as long as the team does the tweaking, the trade-offs are usually manageable (for them).</p> <p>Then I came across this article from The Pragmatic Engineer, talking about some of the common processes used in Big Tech. What's really curious to me is that the process that usually sticks with us is closer to the Plan -&gt; Build -&gt; Ship methods commonly used in these companies, than say, Scrum, or Scrumban: the names we call our tweaked methods.</p>"},{"location":"2024/05/13/the-divs-model/#the-model","title":"The model","text":"<p>The details of the DIVS model is presented here. It means Design, Iterate, Validate, Ship. It's by no means a novel idea; it's a formalization of what we already do, which is common, and familiar. And I don't think this is unique to Alphaus. I'm sure a lot of startups started with Scrum and ended with a revised version but still call it Scrum, or a version of the Plan -&gt; Build -&gt; Ship method but without a \"formal\" name. The DIVS model is our proposal to put a name to that tweaked version.</p> <p>DIVS was designed to be lightweight. Or at least lightweight enough to not get in the way of a startup's preference for speed in development. It's also made to be generic enough to be tweaked as it is intended to be tweaked by teams.</p>"},{"location":"2024/05/13/the-divs-model/#feedback","title":"Feedback","text":"<p>We intentionally put it in a public repository (with an MIT licence) for easier feedback, comments, or modifications. Tell us what you think of it.</p>"},{"location":"2023/04/20/welcome/","title":"Welcome","text":"<p>Welcome to our engineering team blog! We are a group of passionate engineers and developers who are dedicated to creating innovative solutions through our products. We are excited to share our expertise, experiences, insights, and struggles with you through this blog.</p> <p>Through this blog, we aim to provide valuable insights, tutorials, and updates about our products and projects. We will share our experiences, challenges, and lessons learned while developing innovative solutions for the cloud.</p> <p>We welcome your feedback, questions, and ideas for future blog topics. We believe that by sharing knowledge, we can inspire others and drive progress in the tech industry.</p> <p>Thank you for joining us on this exciting journey, and we look forward to sharing our knowledge and experiences with you!</p> <p>In the meantime, check out these related links:</p> <p>Homepage - https://alphaus.cloud/en/ Documentation - https://labs.alphaus.cloud/docs/ Twitter - https://twitter.com/alphauslabs LinkedIn - https://www.linkedin.com/company/alphaus/</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"category/products/","title":"Products","text":""},{"location":"category/infrastructure/","title":"Infrastructure","text":""},{"location":"category/tech/","title":"Tech","text":""},{"location":"category/process/","title":"Process","text":""},{"location":"category/finops/","title":"FinOps","text":""},{"location":"category/ai/","title":"AI","text":""},{"location":"category/cli/","title":"CLI","text":""},{"location":"category/api/","title":"API","text":""},{"location":"category/welcome/","title":"Welcome","text":""},{"location":"author/flowerinthenight/","title":"Chew","text":""}]}